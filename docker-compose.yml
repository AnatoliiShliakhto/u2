services:

# SurrealDB TiKV cluster
#  pd0:
#    container_name: surrealdb-pd0
#    hostname: pd0
#    image: pingcap/pd:latest
#    networks:
#      - intranet
#    volumes:
#      - surrealdb:/data
#      - logs:/logs
#    command:
#      - --name=pd0
#      - --client-urls=http://0.0.0.0:2379
#      - --peer-urls=http://0.0.0.0:2380
#      - --advertise-client-urls=http://pd0:2379
#      - --advertise-peer-urls=http://pd0:2380
#      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380
#      - --data-dir=/data/pd0
#      - --log-file=/logs/surrealdb-pd0.log
#    restart: on-failure
#    healthcheck:
#      test: /pd-ctl health | jq -e ".[] | select(.name == \"$(hostname)\").health"
#      start_period: 5s
#      retries: 5
#      timeout: 10s
#
#  pd1:
#    container_name: surrealdb-pd1
#    hostname: pd1
#    image: pingcap/pd:latest
#    networks:
#      - intranet
#    volumes:
#      - surrealdb:/data
#      - logs:/logs
#    command:
#      - --name=pd1
#      - --client-urls=http://0.0.0.0:2379
#      - --peer-urls=http://0.0.0.0:2380
#      - --advertise-client-urls=http://pd1:2379
#      - --advertise-peer-urls=http://pd1:2380
#      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380
#      - --data-dir=/data/pd1
#      - --log-file=/logs/surrealdb-pd1.log
#    restart: on-failure
#    healthcheck:
#      test: /pd-ctl health | jq -e ".[] | select(.name == \"$(hostname)\").health"
#      start_period: 5s
#      retries: 5
#      timeout: 10s
#
#  tikv0:
#    container_name: surrealdb-tikv0
#    hostname: tikv0
#    image: pingcap/tikv:latest
#    networks:
#      - intranet
#    volumes:
#      - surrealdb:/data
#      - logs:/logs
#    command:
#      - --addr=0.0.0.0:20160
#      - --advertise-addr=tikv0:20160
#      - --status-addr=0.0.0.0:20180
#      - --data-dir=/data/tikv0
#      - --pd=pd0:2379,pd1:2379
#      - --log-file=/logs/surrealdb-tikv0.log
#    depends_on:
#      pd0:
#        condition: service_healthy
#      pd1:
#        condition: service_healthy
#    restart: on-failure
#    healthcheck:
#      test: /tikv-ctl --host $(hostname):20160 metrics
#      start_period: 5s
#      retries: 5
#      timeout: 10s
#
#  tikv1:
#    container_name: surrealdb-tikv1
#    hostname: tikv1
#    image: pingcap/tikv:latest
#    networks:
#      - intranet
#    volumes:
#      - surrealdb:/data
#      - logs:/logs
#    command:
#      - --addr=0.0.0.0:20160
#      - --advertise-addr=tikv1:20160
#      - --status-addr=0.0.0.0:20180
#      - --data-dir=/data/tikv1
#      - --pd=pd0:2379,pd1:2379
#      - --log-file=/logs/surrealdb-tikv1.log
#    depends_on:
#      pd0:
#        condition: service_healthy
#      pd1:
#        condition: service_healthy
#    restart: on-failure
#    healthcheck:
#      test: /tikv-ctl --host $(hostname):20160 metrics
#      start_period: 5s
#      retries: 5
#      timeout: 10s

# SurrealDB

  surrealdb:
    container_name: surrealdb
    image: surrealdb/surrealdb:latest
    user: root
    networks:
      - intranet
    ports:
      - "8000:8000" # debug only
    environment:
      - SURREAL_USER=${DB_ROOT_USER:-root}
      - SURREAL_PASS=${DB_ROOT_PASS:-root}
#      - SURREAL_PATH=tikv://pd0:2379 # TiKV cluster
      - SURREAL_PATH=rocksdb:/data # standalone
    entrypoint:
      - /surreal
      - start
    volumes:
      - surrealdb:/data # standalone
    security_opt:
      - no-new-privileges:true
#    depends_on:
#      tikv0:
#        condition: service_healthy
#      tikv1:
#        condition: service_healthy
    restart: always
    healthcheck:
      test: ["CMD", "/surreal", "is-ready"]
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 5s

# Prometheus

  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    user: nobody
    ports:
      - "9090:9090" # debug only
    networks:
      - intranet
    volumes:
      - prometheus:/prometheus
      - ./cfg/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: wget --no-verbose --tries=1 --spider prometheus:9090/-/healthy || exit 1
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 5s

# RabbitMQ AMQP

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:latest
    networks:
      - intranet
    ports:
#      - "5672:5672"
      - "15672:15672" # debug only
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-root}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASS:-root}
      RABBITMQ_VHOST: ${RABBITMQ_VHOST:-/}
      RABBITMQ_NODENAME: ${RABBITMQ_NODENAME:-rabbitmq}
    volumes:
      - rabbitmq:/var/lib/rabbitmq
    configs:
      - source: rabbitmq-plugins
        target: /etc/rabbitmq/enabled_plugins
      - source: rabbitmq-erlang-cookie
        target: /var/lib/rabbitmq/.erlang.cookie
        mode: 0400
    restart: unless-stopped
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s

# Grafana

  grafana:
    container_name: grafana
    image: grafana/grafana:latest
    ports:
      - "3000:3000" # debug only
    networks:
      - intranet
    volumes:
      - grafana:/var/lib/grafana
      - ./cfg/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./cfg/grafana/provisioning:/etc/grafana/provisioning
      - ./cfg/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-root}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASS:-root}
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_PLUGINS_PREINSTALL=grafana-surrealdb-datasource
      - GF_DOMAIN=${DOMAIN:-localhost}
    depends_on:
      prometheus:
        condition: service_healthy
    security_opt:
      - no-new-privileges:true
    restart: unless-stopped
    healthcheck:
      test: wget --no-verbose --tries=1 --spider grafana:3000/api/health || exit 1
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s

# Caddy reverse proxy

  proxy-srv:
    image: caddy:latest
    restart: unless-stopped
    depends_on:
      access-svc:
        condition: service_started
    networks:
      - intranet
    ports:
      - "443:443"
    volumes:
      - ./cfg/certificates:/etc/ssl/private:ro
      - ./cfg/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
        
  rdesktop:
    image: lscr.io/linuxserver/rdesktop:ubuntu-xfce
    container_name: rdesktop
    security_opt:
      - seccomp:unconfined #optional
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock #optional
      - ./cfg/rdesktop:/config #optional
    ports:
      - "3380:3389"
    networks:
      - intranet
    devices:
      - /dev/dri:/dev/dri #optional
    shm_size: "1gb" #optional
    restart: unless-stopped

# Logger service

  logger-svc:
    container_name: logger-svc
    hostname: logger
    image: alpine:latest
    networks:
      - intranet
    security_opt:
      - no-new-privileges:true
    volumes:
      - ./bin/logger:/logger
      - logs:/logs
    entrypoint: [ "/logger" ]
    environment:
      LOGS_DIR: /logs
      AMQP_URL: amqp://${RABBITMQ_USER:-root}:${RABBITMQ_PASS:-root}@rabbitmq:5672/%2f
    restart: unless-stopped
    depends_on:
      rabbitmq:
        condition: service_started
    healthcheck:
      test: wget --no-verbose --tries=1 $(hostname)/healthcheck || exit 1
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Auth service

  system-svc:
    container_name: system-svc
    hostname: system
    image: alpine:latest
    networks:
      - intranet
    security_opt:
      - no-new-privileges:true
    environment:
      AMQP_URL: amqp://${RABBITMQ_USER:-root}:${RABBITMQ_PASS:-root}@rabbitmq:5672/%2f
      DATA_PATH: ${DATA_PATH:-/etc/u2}
      DB_URL: ${DB_URL:-surrealdb:8000}
      DB_NAMESPACE: ${DB_NAMESPACE:-u2}
      DB_USER: ${DB_ROOT_USER:-root}
      DB_PASS: ${DB_ROOT_PASS:-root}
      SERVICES_DB_CFG: >
        [
          { "database": "${AUTH_DB_NAME:-core}", "user": "${AUTH_DB_USER:-root}", "password": "${AUTH_DB_PASS:-root}" },
          { "database": "${AUDIT_DB_NAME:-core}", "user": "${AUDIT_DB_USER:-root}", "password": "${AUDIT_DB_PASS:-root}" }
        ]
    volumes:
      - ./bin/system:/system
      - ./bin/cfg/system:/etc/u2
    entrypoint: [ "/system" ]
    restart: unless-stopped
    depends_on:
      rabbitmq:
        condition: service_started
      surrealdb:
        condition: service_healthy

# Access service

  access-svc:
    container_name: access-svc
    hostname: access
    image: alpine:latest
    networks:
      - intranet
    security_opt:
      - no-new-privileges:true
    environment:
      AMQP_URL: amqp://${RABBITMQ_USER:-root}:${RABBITMQ_PASS:-root}@rabbitmq:5672/%2f
    volumes:
      - ./bin/access:/access
    entrypoint: [ "/access" ]
    depends_on:
      system-svc:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: wget --no-verbose --tries=1 --spider $(hostname)/healthcheck || exit 1
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s

# Auth service

  auth-svc:
    container_name: auth-svc
    hostname: auth
    image: alpine:latest
    networks:
      - intranet
    security_opt:
      - no-new-privileges:true
    environment:
      AMQP_URL: amqp://${RABBITMQ_USER:-root}:${RABBITMQ_PASS:-root}@rabbitmq:5672/%2f
      DATA_PATH: ${DATA_PATH:-/etc/u2}
      DB_URL: ${DB_URL:-surrealdb:8000}
      DB_NAMESPACE: ${DB_NAMESPACE:-u2}
      DB_DATABASE: ${AUTH_DB_NAME:-core}
      DB_USER: ${AUTH_DB_USER:-root}
      DB_PASS: ${AUTH_DB_PASS:-root}
    volumes:
      - ./bin/auth:/auth
      - ./cfg/auth:/etc/u2
    entrypoint: [ "/auth" ]
    restart: unless-stopped
    depends_on:
      system-svc:
        condition: service_started
    healthcheck:
      test: wget --no-verbose --tries=1 $(hostname)/healthcheck || exit 1
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Audit service

  audit-svc:
    container_name: audit-svc
    hostname: audit
    image: alpine:latest
    networks:
      - intranet
    security_opt:
      - no-new-privileges:true
    environment:
      AMQP_URL: amqp://${RABBITMQ_USER:-root}:${RABBITMQ_PASS:-root}@rabbitmq:5672/%2f
      DATA_PATH: ${DATA_PATH:-/etc/u2}
      DB_URL: ${DB_URL:-surrealdb:8000}
      DB_NAMESPACE: ${DB_NAMESPACE:-u2}
      DB_DATABASE: ${AUDIT_DB_NAME:-core}
      DB_USER: ${AUDIT_DB_USER:-root}
      DB_PASS: ${AUDIT_DB_PASS:-root}
    volumes:
      - ./bin/audit:/audit
      - ./cfg/audit:/etc/u2
    entrypoint: [ "/audit" ]
    restart: unless-stopped
    depends_on:
      system-svc:
        condition: service_started
    healthcheck:
      test: wget --no-verbose --tries=1 $(hostname)/healthcheck || exit 1
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s

# Volumes

volumes:
  logs:
    driver: local
  surrealdb:
    driver: local
  prometheus:
    driver: local
  rabbitmq:
    driver: local
  grafana:
    driver: local

# Networks

networks:
  intranet:
    driver: bridge

# Configs

configs:
  rabbitmq-plugins:
    content: "[rabbitmq_management, rabbitmq_prometheus]."
  rabbitmq-erlang-cookie:
    content: ${RABBITMQ_COOKIE:-secret}